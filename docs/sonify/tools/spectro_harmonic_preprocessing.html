<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>sonify.tools.spectro_harmonic_preprocessing API documentation</title>
<meta name="description" content="Utilities for preprocessing harmonics extraxcted from spectrograms data before it is sent to supercolider.
Feature detection, and assignment of â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sonify.tools.spectro_harmonic_preprocessing</code></h1>
</header>
<section id="section-intro">
<p>Utilities for preprocessing harmonics extraxcted from spectrograms data before it is sent to supercolider.
Feature detection, and assignment of pitches to detected haronic bands</p>
<h2 id="notes">Notes</h2>
<p>This is old and sloppy code that works but could be improved.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Utilities for preprocessing harmonics extraxcted from spectrograms data before it is sent to supercolider. 
Feature detection, and assignment of pitches to detected haronic bands

Notes
-----------------
This is old and sloppy code that works but could be improved.

&#34;&#34;&#34;

from bsl import spectrogram_features
import scipy.ndimage as ndimage
import numpy as np

def n_octa_arpeggio(n,groupID,shift, fundamental=220):
        &#34;&#34;&#34;Determine where in the major chord the passed frequency sits. We also impose a perceptual boundary
        at which the frequency stack of triad notes resets so we don&#39;t have something like the 24th detected band being a triad frequency that
        is exteremly high pitched. 

        
        Parameters
        --------------------
        n : int
                number of octaves we will limit the frequency range to
                This is a perceptual consideration - we don&#39;t want to synthesize frequencies outside of the range of human hearing

        groupID : int
                passed from the feature detection. These correspond to the vertically stacked bands.
                Each representing root, third, or fifth in arpeggio
                e.g., for fundamental 220 some sample groupID inputs are as follows 1,2,3,4,5,6 will give A3, C#4, E4, A4, C#5, E5, respectively

        shift : float 
                for synthesizing glissando like effects when the frequencies falloff.
                See documentation for preprocessing.chroma for more info.

        fundamental : float, optional
                Default = 220. The starting note upon which to build the n octave arpeggio 

        Returns
        --------------------
        freq: float
                frequency at which to sonify the band based on major triad frequencies, bounded within n octaves.

        &#34;&#34;&#34;


        &#39;&#39;&#39;
        this is how we reassign notes that fall above the octave limit.
        so if we have a group id that would be assigned to A10 (arbitrary, any high pitch), this will knock it down to
        something lower, e.g., A5 (these values aren&#39;t tested, just illustrating the concept)
        &#39;&#39;&#39;
        while groupID&gt;n:
            groupID-=n

        &#34;&#34;&#34;
        Converting the triad position to respective chromatic scale position
        E.g., we have 1,2,3 which correspond to root, third, fifth, respectively
        Need to transform these into chromatic scale ids within the correct octave. 
        In general, root, third, fifth correspond to chromatic scale notes 0, 4, 7
        Adustment is made for the fifth as spacing is different
        &#34;&#34;&#34;
        noteID = (groupID-1)*4

        fifths = np.arange(8,100,12)
        if noteID in fifths:
            noteID -=1

        freq = chroma(noteID,shift,fundamental)

        return(freq)

def chroma(noteID,shift,fundamental):
        &#34;&#34;&#34;Compute the frequency of the nth note in the chromatic scale with stating note frequency `fundamental`
        Option to shift the pitch if glissando effect desired.

        Parameters
        ----------------
        noteID : int 
                The nth note of the chromatic scale. E.g., if fundamental is A4 (440 Hz) and 11 is passed for noteID
                the frequency for G#5 will be returned. If 23 is passed, the frequency for G#6 will be returned, etc.

        shift : float
                Allows for defining the frequency of notes in between notes of 12 tone equal temperament chromatic scale.
                For example, if we have fundamental = 440 , noteID = 1, and shift = 0.5, we will get the frequency of the
                note that is between A4 and A#4 (in this case, allows to get the quartertone)

        Fundamental : float
                Starting note of the chromatic scale we are computing. E.g., fundamental = 440 will start chromatic scale at A4 


        Returns
        ------------------
        frequency : float
                The frequency of the requested note on the chromatic scale

        &#34;&#34;&#34;
        if shift == 0:
                return(fundamental*(2**(noteID/12.00)))
        else:
                return(fundamental*(2**((noteID+shift)/12.00)))


def getPitches(blob):
        &#34;&#34;&#34;Once we have detected bands in the spectrogram, detect each unique band and assign it a frequency corresponding to a major triad.

        Parameters
        ------------------------
        blob : 2-D numpy array
                This should be the binarized/skeletonized array of detected harmonics from the spectrogram.


        Returns
        -----------------------
        masked_label : 2-D numpy array
                Array of frequencies to send to supercollider oscillators. Same shape as spectrogram.
                Each detected band is assigned a pitch such that stacked bands will form a major chord. 
                If bands decay in frequency this will be reflected in the changes in pitch (i.e., glissando)

        labeled_array : 2-D numpy array
                Labels frolm ndimage.label. Same shape as spectrogram. Useful for visualizing what groups were detected by the algorithm.
                Not used for sonification, rather as a visual sanity check / generating figures

        &#34;&#34;&#34;

        &#39;&#39;&#39;
        We want  to be able to detect continuous bands, but sometimes there are small gaps in the data, meaning more bands are detected than visually perceived. 
        These gaps would generally not really be considered by the visual system, as it would just look for continuity of bands, however the computer doesnt think this way...
        So the workaround for this is to just dialate every band so that these small discontinuities are filled. 
        There are two possible dialation structures below -&gt; I (Lucas) ended up using structure2 to dialate but feel free to experiment.
        Take a look at the ndimage.binary_deialation docs for more info. 
        &#39;&#39;&#39;
        structure1 = np.array([[1,1,1],
                           [1,1,1],
                           [1,1,1]])

        structure2 = np.array([[0,1,0],
                           [1,1,1],
                           [0,1,0]])


        dial = ndimage.binary_dilation(blob, structure=structure2).astype(blob.dtype)

        &#39;&#39;&#39;
        With ndimage.label we are basically searching for connected pixel regions. Because everything is binarized this is pretty straightforward.
        We can also specify the search structure -&gt; I found thast structure1 works best for this.
        &#39;&#39;&#39;
        labeled_array,num_features = ndimage.label(dial,structure=structure1)
        #masked_label = blob*labeled_array #thin line representations 
        masked_label=labeled_array.copy() #blob representations

        unique_label_dict = {}

        for idx,col in enumerate(masked_label.T):
                nonzero_index = np.nonzero(col)
                label = col[nonzero_index]

                #for each nonzero label
                for j in range(0,len(label)):
                        #if the first instance of the group label
                    if label[j] not in unique_label_dict.keys():

                        #map its group id to its starting row                                                   
                        unique_label_dict[label[j]] =  (nonzero_index[0][j])

                        #assign it an initial frequency based on major triad notes                                      
                        col[nonzero_index[0][j]] = n_octa_arpeggio(9,label[j],shift=0)

                    #else get how far away it is from its inital frequency - e.g, how much the band has decayed after systole             
                    else:
                        #difference is where it is - where it started                                                                           
                        difference = nonzero_index[0][j] - unique_label_dict[label[j]]    
                        #frequency is original freq plus difference # of half steps                     
                        col[nonzero_index[0][j]] = n_octa_arpeggio(9,label[j],shift=0.5*difference)         


        return(masked_label,labeled_array)


def get_Pxx_blob_features(Pxx,freqs):
        &#34;&#34;&#34;Helper function to compute spectrogram features for sonification, and assign sonification parameters to them (e.g., pitch)

        Parameters
        ------------------
        Pxx : 2-D numpy array
                Generally a spectrogram - featrues will be detected on this
        freqs : 1-D numpy ARRAY
                Frequency bins from the fft 

        Returns
        ----------------
        active : 2-D numpy array
                Same shape as spectrogram - will either contain a 1 if harmonics were detected at that point or a 0 otherwise
                Used to trun on/of the corresponding oscillator in supercollider

        envelope : 1-D numpy array
                Frequencies corresponding to outer edge of spectrogram

        pitches : 2-D numpy array
                Same shape as spectrogram. Same non-zero entries as active (i.e., non-zero where harmonics are detected)
                Used to set the frequency of the oscillator that is turned on by active

        plotLavels: 2-D numpy array
                Haven&#39;t used this in some time - dimensions should be the same as the spectrogram.
                See preprocessing.getPitches documentation for labeled_array
                I am pretty sure this assigns a unique ID number to each detected spectrogram band, so when plotted you can see how the algorithm has detected each group 
        
        relativePeakHeight : 2-D numpy array
                Same shape as spectrogram. For the peaks detected in each time bin power trace, get their relative height with respect to the interpolated power falloff

        &#34;&#34;&#34;
        envelope = spectrogram_features.get_envelope(Pxx.copy(),freqs)

        bands,relativePeakHeight, extrapolated_envelope = spectrogram_features.get_bands_and_extrapolation(Pxx.copy(),freqs)
        active = np.where(bands&lt;0,1,0)

        pitches, plotLabels = getPitches(active)

        return active, envelope, pitches, plotLabels, relativePeakHeight</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sonify.tools.spectro_harmonic_preprocessing.chroma"><code class="name flex">
<span>def <span class="ident">chroma</span></span>(<span>noteID, shift, fundamental)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the frequency of the nth note in the chromatic scale with stating note frequency <code>fundamental</code>
Option to shift the pitch if glissando effect desired.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>noteID</code></strong> :&ensp;<code>int </code></dt>
<dd>The nth note of the chromatic scale. E.g., if fundamental is A4 (440 Hz) and 11 is passed for noteID
the frequency for G#5 will be returned. If 23 is passed, the frequency for G#6 will be returned, etc.</dd>
<dt><strong><code>shift</code></strong> :&ensp;<code>float</code></dt>
<dd>Allows for defining the frequency of notes in between notes of 12 tone equal temperament chromatic scale.
For example, if we have fundamental = 440 , noteID = 1, and shift = 0.5, we will get the frequency of the
note that is between A4 and A#4 (in this case, allows to get the quartertone)</dd>
<dt><strong><code>Fundamental</code></strong> :&ensp;<code>float</code></dt>
<dd>Starting note of the chromatic scale we are computing. E.g., fundamental = 440 will start chromatic scale at A4</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>frequency</code></strong> :&ensp;<code>float</code></dt>
<dd>The frequency of the requested note on the chromatic scale</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def chroma(noteID,shift,fundamental):
        &#34;&#34;&#34;Compute the frequency of the nth note in the chromatic scale with stating note frequency `fundamental`
        Option to shift the pitch if glissando effect desired.

        Parameters
        ----------------
        noteID : int 
                The nth note of the chromatic scale. E.g., if fundamental is A4 (440 Hz) and 11 is passed for noteID
                the frequency for G#5 will be returned. If 23 is passed, the frequency for G#6 will be returned, etc.

        shift : float
                Allows for defining the frequency of notes in between notes of 12 tone equal temperament chromatic scale.
                For example, if we have fundamental = 440 , noteID = 1, and shift = 0.5, we will get the frequency of the
                note that is between A4 and A#4 (in this case, allows to get the quartertone)

        Fundamental : float
                Starting note of the chromatic scale we are computing. E.g., fundamental = 440 will start chromatic scale at A4 


        Returns
        ------------------
        frequency : float
                The frequency of the requested note on the chromatic scale

        &#34;&#34;&#34;
        if shift == 0:
                return(fundamental*(2**(noteID/12.00)))
        else:
                return(fundamental*(2**((noteID+shift)/12.00)))</code></pre>
</details>
</dd>
<dt id="sonify.tools.spectro_harmonic_preprocessing.getPitches"><code class="name flex">
<span>def <span class="ident">getPitches</span></span>(<span>blob)</span>
</code></dt>
<dd>
<div class="desc"><p>Once we have detected bands in the spectrogram, detect each unique band and assign it a frequency corresponding to a major triad.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>blob</code></strong> :&ensp;<code>2-D numpy array</code></dt>
<dd>This should be the binarized/skeletonized array of detected harmonics from the spectrogram.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>masked_label</code></strong> :&ensp;<code>2-D numpy array</code></dt>
<dd>Array of frequencies to send to supercollider oscillators. Same shape as spectrogram.
Each detected band is assigned a pitch such that stacked bands will form a major chord.
If bands decay in frequency this will be reflected in the changes in pitch (i.e., glissando)</dd>
<dt><strong><code>labeled_array</code></strong> :&ensp;<code>2-D numpy array</code></dt>
<dd>Labels frolm ndimage.label. Same shape as spectrogram. Useful for visualizing what groups were detected by the algorithm.
Not used for sonification, rather as a visual sanity check / generating figures</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getPitches(blob):
        &#34;&#34;&#34;Once we have detected bands in the spectrogram, detect each unique band and assign it a frequency corresponding to a major triad.

        Parameters
        ------------------------
        blob : 2-D numpy array
                This should be the binarized/skeletonized array of detected harmonics from the spectrogram.


        Returns
        -----------------------
        masked_label : 2-D numpy array
                Array of frequencies to send to supercollider oscillators. Same shape as spectrogram.
                Each detected band is assigned a pitch such that stacked bands will form a major chord. 
                If bands decay in frequency this will be reflected in the changes in pitch (i.e., glissando)

        labeled_array : 2-D numpy array
                Labels frolm ndimage.label. Same shape as spectrogram. Useful for visualizing what groups were detected by the algorithm.
                Not used for sonification, rather as a visual sanity check / generating figures

        &#34;&#34;&#34;

        &#39;&#39;&#39;
        We want  to be able to detect continuous bands, but sometimes there are small gaps in the data, meaning more bands are detected than visually perceived. 
        These gaps would generally not really be considered by the visual system, as it would just look for continuity of bands, however the computer doesnt think this way...
        So the workaround for this is to just dialate every band so that these small discontinuities are filled. 
        There are two possible dialation structures below -&gt; I (Lucas) ended up using structure2 to dialate but feel free to experiment.
        Take a look at the ndimage.binary_deialation docs for more info. 
        &#39;&#39;&#39;
        structure1 = np.array([[1,1,1],
                           [1,1,1],
                           [1,1,1]])

        structure2 = np.array([[0,1,0],
                           [1,1,1],
                           [0,1,0]])


        dial = ndimage.binary_dilation(blob, structure=structure2).astype(blob.dtype)

        &#39;&#39;&#39;
        With ndimage.label we are basically searching for connected pixel regions. Because everything is binarized this is pretty straightforward.
        We can also specify the search structure -&gt; I found thast structure1 works best for this.
        &#39;&#39;&#39;
        labeled_array,num_features = ndimage.label(dial,structure=structure1)
        #masked_label = blob*labeled_array #thin line representations 
        masked_label=labeled_array.copy() #blob representations

        unique_label_dict = {}

        for idx,col in enumerate(masked_label.T):
                nonzero_index = np.nonzero(col)
                label = col[nonzero_index]

                #for each nonzero label
                for j in range(0,len(label)):
                        #if the first instance of the group label
                    if label[j] not in unique_label_dict.keys():

                        #map its group id to its starting row                                                   
                        unique_label_dict[label[j]] =  (nonzero_index[0][j])

                        #assign it an initial frequency based on major triad notes                                      
                        col[nonzero_index[0][j]] = n_octa_arpeggio(9,label[j],shift=0)

                    #else get how far away it is from its inital frequency - e.g, how much the band has decayed after systole             
                    else:
                        #difference is where it is - where it started                                                                           
                        difference = nonzero_index[0][j] - unique_label_dict[label[j]]    
                        #frequency is original freq plus difference # of half steps                     
                        col[nonzero_index[0][j]] = n_octa_arpeggio(9,label[j],shift=0.5*difference)         


        return(masked_label,labeled_array)</code></pre>
</details>
</dd>
<dt id="sonify.tools.spectro_harmonic_preprocessing.get_Pxx_blob_features"><code class="name flex">
<span>def <span class="ident">get_Pxx_blob_features</span></span>(<span>Pxx, freqs)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function to compute spectrogram features for sonification, and assign sonification parameters to them (e.g., pitch)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>Pxx</code></strong> :&ensp;<code>2-D numpy array</code></dt>
<dd>Generally a spectrogram - featrues will be detected on this</dd>
<dt><strong><code>freqs</code></strong> :&ensp;<code>1-D numpy ARRAY</code></dt>
<dd>Frequency bins from the fft</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>active</code></strong> :&ensp;<code>2-D numpy array</code></dt>
<dd>Same shape as spectrogram - will either contain a 1 if harmonics were detected at that point or a 0 otherwise
Used to trun on/of the corresponding oscillator in supercollider</dd>
<dt><strong><code>envelope</code></strong> :&ensp;<code>1-D numpy array</code></dt>
<dd>Frequencies corresponding to outer edge of spectrogram</dd>
<dt><strong><code>pitches</code></strong> :&ensp;<code>2-D numpy array</code></dt>
<dd>Same shape as spectrogram. Same non-zero entries as active (i.e., non-zero where harmonics are detected)
Used to set the frequency of the oscillator that is turned on by active</dd>
<dt><strong><code>plotLavels</code></strong> :&ensp;<code>2-D numpy array</code></dt>
<dd>Haven't used this in some time - dimensions should be the same as the spectrogram.
See preprocessing.getPitches documentation for labeled_array
I am pretty sure this assigns a unique ID number to each detected spectrogram band, so when plotted you can see how the algorithm has detected each group</dd>
<dt><strong><code>relativePeakHeight</code></strong> :&ensp;<code>2-D numpy array</code></dt>
<dd>Same shape as spectrogram. For the peaks detected in each time bin power trace, get their relative height with respect to the interpolated power falloff</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_Pxx_blob_features(Pxx,freqs):
        &#34;&#34;&#34;Helper function to compute spectrogram features for sonification, and assign sonification parameters to them (e.g., pitch)

        Parameters
        ------------------
        Pxx : 2-D numpy array
                Generally a spectrogram - featrues will be detected on this
        freqs : 1-D numpy ARRAY
                Frequency bins from the fft 

        Returns
        ----------------
        active : 2-D numpy array
                Same shape as spectrogram - will either contain a 1 if harmonics were detected at that point or a 0 otherwise
                Used to trun on/of the corresponding oscillator in supercollider

        envelope : 1-D numpy array
                Frequencies corresponding to outer edge of spectrogram

        pitches : 2-D numpy array
                Same shape as spectrogram. Same non-zero entries as active (i.e., non-zero where harmonics are detected)
                Used to set the frequency of the oscillator that is turned on by active

        plotLavels: 2-D numpy array
                Haven&#39;t used this in some time - dimensions should be the same as the spectrogram.
                See preprocessing.getPitches documentation for labeled_array
                I am pretty sure this assigns a unique ID number to each detected spectrogram band, so when plotted you can see how the algorithm has detected each group 
        
        relativePeakHeight : 2-D numpy array
                Same shape as spectrogram. For the peaks detected in each time bin power trace, get their relative height with respect to the interpolated power falloff

        &#34;&#34;&#34;
        envelope = spectrogram_features.get_envelope(Pxx.copy(),freqs)

        bands,relativePeakHeight, extrapolated_envelope = spectrogram_features.get_bands_and_extrapolation(Pxx.copy(),freqs)
        active = np.where(bands&lt;0,1,0)

        pitches, plotLabels = getPitches(active)

        return active, envelope, pitches, plotLabels, relativePeakHeight</code></pre>
</details>
</dd>
<dt id="sonify.tools.spectro_harmonic_preprocessing.n_octa_arpeggio"><code class="name flex">
<span>def <span class="ident">n_octa_arpeggio</span></span>(<span>n, groupID, shift, fundamental=220)</span>
</code></dt>
<dd>
<div class="desc"><p>Determine where in the major chord the passed frequency sits. We also impose a perceptual boundary
at which the frequency stack of triad notes resets so we don't have something like the 24th detected band being a triad frequency that
is exteremly high pitched. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>number of octaves we will limit the frequency range to
This is a perceptual consideration - we don't want to synthesize frequencies outside of the range of human hearing</dd>
<dt><strong><code>groupID</code></strong> :&ensp;<code>int</code></dt>
<dd>passed from the feature detection. These correspond to the vertically stacked bands.
Each representing root, third, or fifth in arpeggio
e.g., for fundamental 220 some sample groupID inputs are as follows 1,2,3,4,5,6 will give A3, C#4, E4, A4, C#5, E5, respectively</dd>
<dt><strong><code>shift</code></strong> :&ensp;<code>float </code></dt>
<dd>for synthesizing glissando like effects when the frequencies falloff.
See documentation for preprocessing.chroma for more info.</dd>
<dt><strong><code>fundamental</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Default = 220. The starting note upon which to build the n octave arpeggio</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>freq</code></strong> :&ensp;<code>float</code></dt>
<dd>frequency at which to sonify the band based on major triad frequencies, bounded within n octaves.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def n_octa_arpeggio(n,groupID,shift, fundamental=220):
        &#34;&#34;&#34;Determine where in the major chord the passed frequency sits. We also impose a perceptual boundary
        at which the frequency stack of triad notes resets so we don&#39;t have something like the 24th detected band being a triad frequency that
        is exteremly high pitched. 

        
        Parameters
        --------------------
        n : int
                number of octaves we will limit the frequency range to
                This is a perceptual consideration - we don&#39;t want to synthesize frequencies outside of the range of human hearing

        groupID : int
                passed from the feature detection. These correspond to the vertically stacked bands.
                Each representing root, third, or fifth in arpeggio
                e.g., for fundamental 220 some sample groupID inputs are as follows 1,2,3,4,5,6 will give A3, C#4, E4, A4, C#5, E5, respectively

        shift : float 
                for synthesizing glissando like effects when the frequencies falloff.
                See documentation for preprocessing.chroma for more info.

        fundamental : float, optional
                Default = 220. The starting note upon which to build the n octave arpeggio 

        Returns
        --------------------
        freq: float
                frequency at which to sonify the band based on major triad frequencies, bounded within n octaves.

        &#34;&#34;&#34;


        &#39;&#39;&#39;
        this is how we reassign notes that fall above the octave limit.
        so if we have a group id that would be assigned to A10 (arbitrary, any high pitch), this will knock it down to
        something lower, e.g., A5 (these values aren&#39;t tested, just illustrating the concept)
        &#39;&#39;&#39;
        while groupID&gt;n:
            groupID-=n

        &#34;&#34;&#34;
        Converting the triad position to respective chromatic scale position
        E.g., we have 1,2,3 which correspond to root, third, fifth, respectively
        Need to transform these into chromatic scale ids within the correct octave. 
        In general, root, third, fifth correspond to chromatic scale notes 0, 4, 7
        Adustment is made for the fifth as spacing is different
        &#34;&#34;&#34;
        noteID = (groupID-1)*4

        fifths = np.arange(8,100,12)
        if noteID in fifths:
            noteID -=1

        freq = chroma(noteID,shift,fundamental)

        return(freq)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#notes">Notes</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sonify.tools" href="index.html">sonify.tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sonify.tools.spectro_harmonic_preprocessing.chroma" href="#sonify.tools.spectro_harmonic_preprocessing.chroma">chroma</a></code></li>
<li><code><a title="sonify.tools.spectro_harmonic_preprocessing.getPitches" href="#sonify.tools.spectro_harmonic_preprocessing.getPitches">getPitches</a></code></li>
<li><code><a title="sonify.tools.spectro_harmonic_preprocessing.get_Pxx_blob_features" href="#sonify.tools.spectro_harmonic_preprocessing.get_Pxx_blob_features">get_Pxx_blob_features</a></code></li>
<li><code><a title="sonify.tools.spectro_harmonic_preprocessing.n_octa_arpeggio" href="#sonify.tools.spectro_harmonic_preprocessing.n_octa_arpeggio">n_octa_arpeggio</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>